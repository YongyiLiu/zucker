<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[支付渠道架构]]></title>
    <url>%2F2017%2F07%2F31%2Fpay%2F</url>
    <content type="text"><![CDATA[###注意的坑1.请求设置超时2.状态码设计：首先以业务结果码为准，在业务结果为失败时，再去检查明细错误码。3. ##推荐文档支付网关的设计支付系统整体架构大众点评支付渠道网关系统的实践之路去哪儿网支付系统架构演进]]></content>
      <categories>
        <category>chonggou</category>
      </categories>
      <tags>
        <tag>pay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s相关命令]]></title>
    <url>%2F2017%2F07%2F27%2Fk8s1%2F</url>
    <content type="text"><![CDATA[kubectl get eventskubectl get pods -l run=my-nginx -o widekubectl describe svc pod rckubectl clusterinfo dumpkubectl get deploy kube-dns -o yamlkubectl taint 192.168.56.101 kubectl config use-context default-system kubectl get current-contextkubectl get current-clusterkubectl delete deploy kube-dns kubectl get sa 3936 kubectl get sa kube-dns 3937 kubectl get sa kube-dns -o yaml kubectl logs kube-dns-3097350089-c06sx kubedns]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Ubuntu上kubebetes搭建]]></title>
    <url>%2F2017%2F07%2F23%2Fk8s%2F</url>
    <content type="text"><![CDATA[kubernetes拥有并行启动，自动扩容，有差别的批量处理和服务生命周期，负载均衡等特性. 下文将介绍k8s(kubernetes)的手动部署过程，更好地了解和学习k8s各个组件。 前期准备这里准备的Ubuntu必须Ubuntu 16.04.2以上，因为k8s需要最低的版本为Ubuntu 16.04以上.其中kuberneter的版本为1.7.1。 注意：etcd 必须3.0版本以上，因为kuber-apiserver默认的ectd配置是3.0版本，不然需要手动指定 下文安装k8s必须有三台机器，分别为master、node1和node2,分别对应的ip为：192.168.56.101、192.168.56.102和192.168.56.103 通过vagrant快速安装虚拟机（暂不建议采用这种方式，因为后期修改虚拟机配置文件后，会导致ssh连接不上去）下面介绍的是在mac环境中，通过brew命令安装虚拟机的过程，如果mac里面没有安装brew的话，可以参考brew官方文档去安装，在此就不作详细介绍。对于其他环境请到其他官方网站下载进行安装。 1.安装Virtualbox 可以通过一下命令安装Virtualbox 1brew cask install virtualbox 2.安装Vagrant 可以通过一下命令安装Vagrant 1brew cask install vagrant 3.下载box 安装好Virtualbox和vagrant后，准备下一步安装box，相关box版本信息可见Vgrant Cloud 可以通过下面命令进行安装box 1vagrant box add ubuntu/xenial64 安装之后通过下面命令可以获取相关版本信息 1vagrant box list 3.安装虚拟机并启动 在某目录下，分别建立master、node1和node2目录，并依次进入刚刚新建目录中执行下面命令 12vagrant init ubuntu/xenial64vagrant up 安装完成后，在当前目录（必须是当前目录）可以通过下面命令登录进入机器 1vagrant ssh 如果想修改相关机器的配置信息，例如：IP地址、内存等，可以通过修改当前目录下的Vagrantfile文件 也可以通过其他方式(ssh)登录进去，先获取key,接着通过-i参数指定key 12vagrant ssh-configssh -p Port hostname@ip -i IdentityFile etcd配置统一在三台机器上切换到root用户下进行配置 1.下载etcd 依次在三台机器上面下载etcd，并进行解压 123456ETCD_VERSION=$&#123;ETCD_VERSION:-"3.2.5"&#125;ETCD="etcd-v$&#123;ETCD_VERSION&#125;-linux-amd64"curl -L https://github.com/coreos/etcd/releases/download/v$&#123;ETCD_VERSION&#125;/$&#123;ETCD&#125;.tar.gz -o etcd.tar.gztar xzf etcd.tar.gz -C /tmpmv /tmp/etcd-v$&#123;ETCD_VERSION&#125;-linux-amd64 /opt/bin/ 2.配置ectd在每台机器上面创建/opt/config/etcd.conf和/lib/systemd/system/etcd.service文件并进行一下配置， 先新建config文件 12sudo mkdir -p /var/lib/etcd/sudo mkdir -p /opt/config/ 在/opt/config/etcd.conf文件中： 123456789ETCD_DATA_DIR=/var/lib/etcdETCD_NAME="master"ETCD_INITIAL_CLUSTER="master=http://192.168.56.101:2380,node1=http://192.168.56.102:2380,node2=http://192.168.56.103:2380"ETCD_INITIAL_CLUSTER_STATE=newETCD_LISTEN_PEER_URLS=http://192.168.56.101:2380ETCD_INITIAL_ADVERTISE_PEER_URLS=http://192.168.56.101:2380ETCD_ADVERTISE_CLIENT_URLS=http://192.168.56.101:2379ETCD_LISTEN_CLIENT_URLS=http://192.168.56.101:2379,http://127.0.0.1:2379 注意：ETCD_NAME为主机名称，ETCD_INITIAL_CLUSTER需要配各个主机名称和ip,其他信息需要对应本主机的ip,切勿出错 在/opt/systemd/system/etcd.service文件中： 1234567891011121314151617[Unit]Description=Etcd ServerDocumentation=https://github.com/coreos/etcdAfter=network.target[Service]User=rootType=simpleEnvironmentFile=-/opt/config/etcd.confExecStart=/opt/bin/etcdRestart=on-failureRestartSec=10sLimitNOFILE=40000[Install]WantedBy=multi-user.target 上面文件在每台机器上的配置文件基本一致 3.启动并测试etcd 在每台机器配置完成后，依次执行下面命令 1234systemctl daemon-reload systemctl enable etcdsystemctl start etcd 注意：首次启动后，每次进行修改etcd后，除了每次systemctl daemon-reload，还需systemctl start etcd 执行完成后，通过下面的命令进行测试etcd配置的正确性 123456/opt/bin/etcdctl cluster-healthmember ca933ab8cfffe553 is healthy: got healthy result from http://192.168.56.101:2379member d44832212a08c43f is healthy: got healthy result from http://192.168.56.103:2379member f63afbe816fb463d is healthy: got healthy result from http://192.168.56.102:2379cluster is healthy flanneld配置1.下载flanneld 依次在三台机器上面下载etcd，并进行解压 123curl -L https://github.com/coreos/flannel/releases/download/v0.8.0/flannel-v0.8.0-linux-amd64.tar.gz flannel.tar.gztar xzf flannel.tar.gz -C /tmpmv /tmp/flanneld /opt/bin/ 2.配置flanneld 依次在三台机器上，创建/lib/systemd/system/flanneld.service文件 12345678910111213141516[Unit]Description=FlanneldDocumentation=https://github.com/coreos/flannelAfter=network.targetBefore=docker.service[Service]User=rootExecStart=/opt/bin/flanneld \ --etcd-endpoints="http://192.168.56.101:2379,http://192.168.56.102:2379,http://192.168.56.103:2379" \ --iface=192.168.56.101 \ --ip-masqRestart=on-failureType=notifyLimitNOFILE=65536 注意：iface为当前的机器的ip 3.启动并测试flanneld 执行下面命令进行配置flanneld的ip 1/opt/bin/etcdctl --endpoints="http://192.168.56.101:2379,http://192.168.56.102:2379,http://192.168.56.103:2379" mk /coreos.com/network/config \ '&#123;"Network":"10.1.0.0/16", "Backend": &#123;"Type": "vxlan"&#125;&#125;' 注意：这里命令可能会提示Backend已经存在，但是依然为空,可通过以下方式去解决 12/opt/bin/etcdctl set /coreos.com/network/config '&#123;"Network":"10.1.0.0/16", "Backend": &#123;"Type": "vxlan"&#125;&#125;'/opt/bin/etcdctl get /coreos.com/network/config 进行启动和测试: 123systemctl daemon-reloadsystemctl enable flanneldsystemctl start flanneld docker 配置配置docker,必须保证docker环境已经安装好，可参考docker在Ubuntu安装教程 1.配置 找到flanneld解压出来的mk-docker-opts.sh复制到/user/bin/文件夹下面，并在每台主机执行以下命令123mk-docker-opts.sh -isource /run/flannel/subnet.envifconfig docker0 $&#123;FLANNEL_SUBNET&#125; 为了持久化配置信息，需要在lib/systemd/system/docker.service的EnvironmentFile和ExecStart变量替换下面配置信息 123EnvironmentFile=/var/run/flannel/subnet.envExecStart=/usr/bin/dockerd -g /data/docker --bip=$&#123;FLANNEL_SUBNET&#125; --mtu=$&#123;FLANNEL_MTU&#125;ExecReload=/bin/kill -s HUP $MAINPID 2.重置iptables（针对master主机） 为了避免iptable规则的限制，导致主机容器之间ping不通，需要重置iptables规则123456iptables -Fiptables -Xiptables -Ziptables -P INPUT ACCEPTiptables -P OUTPUT ACCEPTiptables -P FORWARD ACCEPT 3.启动和测试启动123systemctl daemon-reloadsystemctl enable dockersystemctl restart docker 测试 最简单的方法是：在不同的主机上面启动两个容器，然后在容器里面进行ping操作 master配置在master配置中涉及kube-apiserver、kuber-controller和kuber-scheduler三个组件的配置 1.下载 在每台机器使用命令进行下载并进行解压,下载官方库 123curl -L https://dl.k8s.io/v1.7.2/kubernetes-server-linux-amd64.tar.gz kuber.tar.gztar xzf kuber.tar.gz -C /tmpmv /tmp/* /opt/bin/ 2.配置 kube-apiserver配置在master主机上，创建/lib/systemd/system/kube-apiserver.service文件，内容如下： 1234567891011121314151617181920212223[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]User=rootExecStart=/opt/bin/kube-apiserver \ --insecure-bind-address=0.0.0.0 \ --insecure-port=8080 \ --etcd-servers=http://192.168.56.101:2379, http://192.168.56.102:2379, http://192.168.56.103:2379 \ --logtostderr=true \ --allow-privileged=false \ --service-cluster-ip-range=192.1.0.0/16 \ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,SecurityContextDeny,ResourceQuota \ --service-node-port-range=30000-32767 \ --advertise-address=192.168.56.101 \Restart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target 注意：etcd-servers均为各台机器的ip,advertise-address为master的ip地址，service-cluster-ip-range为pod(service)ip地址，可以自行设置 kuber-controller配置在master主机上，创建/lib/systemd/system/kube-controller-manager.service文件，内容如 1234567891011121314[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]User=rootExecStart=/opt/bin/kube-controller-manager \ --master=192.168.56.101:8080 \ --logtostderr=trueRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target kuber-scheduler配置在master主机上，创建/lib/systemd/system/kube-scheduler.service文件，内容如下123456789101112131415[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]User=rootExecStart=/opt/bin/kube-scheduler \ --logtostderr=true \ --master=192.168.56.10:8080Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 3.启动和测试 启动12345678910systemctl daemon-reloadsystemctl enable kube-apiserversystemctl enable kube-controller-managersystemctl enable kube-schedulersystemctl enable flanneldsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-scheduler 测试 通过以下命令进行查看组件启动的状态 123systemctl status kube-apiserversystemctl status kube-controller-managersystemctl status kube-scheduler 注意：同时出现其他错误也可以结合以下命令查看其他日志信息1journalctl -xe 注意：在修改完成其他配置信息后，也是需要通过以下命令才能生效12systemctl daemon-reloadsystemctl restart XXXXX node节点配置在各个node节点中包括kubelet和kube-proxy两个组件的配置 1.配置 kubelet配置在node1和node2中进行创建/lib/systemd/system/kubelet.service，并修改123456789101112131415[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]ExecStart=/opt/bin/kubelet \ --hostname-override=192.168.56.102 \ --api-servers=http://192.168.56.101:8080 \ --logtostderr=trueRestart=on-failureKillMode=process[Install]WantedBy=multi-user.target 注意：hostname-override 为本机的ip,api-servers为master的ip kube-proxy配置在node1和node2中进行创建/lib/systemd/system/kube-proxy.service，并修改12345678910111213[Unit]Description=Kubernetes ProxyAfter=network.target[Service]ExecStart=/opt/bin/kube-proxy \ --hostname-override=192.168.56.102 \ --master=http://192.168.56.101:8080 \ --logtostderr=trueRestart=on-failure[Install]WantedBy=multi-user.target 注意：hostname-override为本机ip,master为master的ip 2.启动和测试 启动12345systemctl daemon-reloadsystemctl enable kubeletsystemctl enable kube-proxysystemctl start kubeletsystemctl start kube-proxy 测试 通过以下命令进行查看组件启动的状态 12systemctl status kubeletsystemctl status kube-proxy 注意：其调试和重启以及调试方式跟上面一致 kubectl客户端配置与测试kubernete集群1.kubectl客户端配置可以通过下面的命令对kubectl进行配置 12345678910111213KUBE_USER=adminKUBE_PASSWORD=adminKUBECONFIG="$&#123;HOME&#125;/.kube/config"mkdir -p $(dirname "$&#123;KUBECONFIG&#125;")touch "$&#123;KUBECONFIG&#125;"CONTEXT=ubuntuKUBECONFIG=$&#123;KUBECONFIG:-$DEFAULT_KUBECONFIG&#125;KUBECONFIG="$&#123;KUBECONFIG&#125;" /tmp/kubernetes/server/bin/kubectl config set-cluster "$&#123;CONTEXT&#125;" --server=https://192.168.56.101:6443 --insecure-skip-tls-verify=trueKUBECONFIG="$&#123;KUBECONFIG&#125;" /tmp/kubernetes/server/bin/kubectl config set-credentials "$&#123;CONTEXT&#125;" --username=$&#123;KUBE_USER&#125; --password=$&#123;KUBE_PASSWORD&#125;KUBECONFIG="$&#123;KUBECONFIG&#125;" /tmp/kubernetes/server/bin/kubectl config set-context "$&#123;CONTEXT&#125;" --cluster="$&#123;CONTEXT&#125;" --user="$&#123;CONTEXT&#125;"KUBECONFIG="$&#123;KUBECONFIG&#125;" /tmp/kubernetes/server/bin/kubectl config use-context "$&#123;CONTEXT&#125;" --cluster="$&#123;CONTEXT&#125;" 注意：需要在node中任意一个节点里面进行配置 2.kubernete集群测试到了这一步，我们已经成功了一半了，下面测试集群的运作是否正常。 获取节点状态12345kubectl get nodesNAME STATUS AGE VERSION192.168.56.101 NotReady,SchedulingDisabled 3d v1.7.1192.168.56.102 Ready 2d v1.7.1192.168.56.103 Ready 2d v1.7.1 如果可以看到上面信息，证明基本正常 使用yml创建任务测试创建nginx.yml文件，其配置信息入下： 12345678910111213141516apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: my-nginxspec: replicas: 2 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 通过以下命令进行创建任务，并测试123456kubectl create -f nginx.ymlkubectl expose deployment/my-nginxkubectl get service my-nginxNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEmy-nginx1 192.1.21.110 &lt;none&gt; 80/TCP 1dcurl http://192.1.21.110 根据提示的信息去访问nignx，如果可以走通的话，恭喜你，已经成功了一半！！ 证书生成接下来每一步都是很艰难，很关键，巨大的坑在前面！！ master证书生成1.创建一个master_ssl.cnf文件，其配置信息如下： 12345678910111213141516[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = kubernetesDNS.2 = kubernetes.defaultDNS.3 = kubernetes.default.svcDNS.4 = kubernetes.default.svc.cluster.localDNS.5 = masterIP.1 = 192.1.0.1IP.2 = 192.168.56.101 注意：IP.1为虚拟服务IP，IP.2为master主机地址 2.生成master证书 执行下面命令生成证书12345678openssl genrsa -out ca.key 2048openssl req -x509 -new -nodes -key ca.key -subj "/CN=company.com" -days 10000 -out ca.crtopenssl genrsa -out server.key 2048openssl req -new -key server.key -subj "/CN=master" -config master_ssl.cnf -out server.csropenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 10000 -extensions v3_req -extfile master_ssl.cnf -out server.crtopenssl genrsa -out client.key 2048openssl req -new -key client.key -subj "/CN=node" -out client.csropenssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 10000 注意：生成的秘钥信息在/root/key/中,下面配置中需要用到这个秘钥s存储的地址 各个node证书生成 接着在各台node,将master中的ca.crt文件和ca.key文件拷贝到各个node节点上，分别执行以下命令,CLINET_IP为当前主机的ip地址 1234CLINET_IP=192.168.56.102openssl genrsa -out client.key 2048openssl req -new -key client.key -subj "/CN=$&#123;CLINET_IP&#125;" -out client.csropenssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 10000 注意：生成的秘钥信息在/root/key/中,下面配置中需要用到这个秘钥s存储的地址 用户认证配置在每台机器（master、node1和node2）中的/etc/kubernetes/目录下创建kubeconfig文件，配置信息如下：123456789101112131415161718apiVersion: v1clusters:- cluster: certificate-authority: /root/key/ca.crt name: ubuntucontexts:- context: cluster: ubuntu user: ubuntu name: ubuntucurrent-context: ubuntukind: Configpreferences: &#123;&#125;users:- name: ubuntu user: client-certificate: /root/key/client.crt client-key: /root/key/client.key 注意层次关系，切勿弄乱或者名称错，否则一直提示，server ask client for certificate… 1.master配置文件的修改 kube-apiserver配置的修改12345678910111213141516171819202122232425[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]User=rootExecStart=/opt/bin/kube-apiserver \ --secure-port=6443 \ --etcd-servers=http://192.168.56.101:2379,http://192.168.56.102:2379,http://192.168.56.103:2379 \ --logtostderr=true \ --allow-privileged=false \ --service-cluster-ip-range=192.1.0.0/16 \ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,SecurityContextDeny,ResourceQuota \ --service-node-port-range=30000-32767 \ --advertise-address=192.168.56.101 \ --client-ca-file=/root/key/ca.crt \ --tls-cert-file=/root/key/server.crt \ --tls-private-key-file=/root/key/server.keyRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target kube-controller-manager配置的修改1234567891011121314151617[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]User=rootExecStart=/opt/bin/kube-controller-manager \ --master=https://192.168.56.101:6443 \ --root-ca-file=/root/key/ca.crt \ --service-account-private-key-file=/root/key/server.key \ --kubeconfig=/etc/kubernetes/kubeconfig \ --logtostderr=trueRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target kuber-scheduler配置的修改123456789101112131415[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]User=rootExecStart=/opt/bin/kube-scheduler \ --logtostderr=true \ --master=https://192.168.56.101:6443 \ --kubeconfig=/etc/kubernetes/kubeconfigRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 2.各个node配置文件的修改 如下是node1节点的配置信息，node2节点依次配置kubele配置的修改12345678910111213141516171819202122[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]ExecStart=/opt/bin/kubelet \ --hostname-override=192.168.56.102 \ --api-servers=https://192.168.56.101:6443 \ --cluster-domain=cluster.local \ --log-dir=/var/log/kubernetes \ --cluster-dns=192.1.0.100 \ --kubeconfig=/etc/kubernetes/kubeconfig \ --logtostderr=trueRestart=on-failureKillMode=process[Install]WantedBy=multi-user.target[Unit]Description=Kubernetes ProxyAfter=network.target kube-proxy配置的修改123456789101112131415[Unit]Description=Kubernetes ProxyAfter=network.target[Service]ExecStart=/opt/bin/kube-proxy \ --hostname-override=192.168.56.102 \ --master=https://192.168.56.101:6443 \ --log-dir=/var/log/kubernetes \ --kubeconfig=/etc/kubernetes/kubeconfig \ --logtostderr=trueRestart=on-failure[Install]WantedBy=multi-user.target dns配置1.创建配置文件创建kubedns-svc.yamll123456789101112131415161718192021apiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS"spec: selector: k8s-app: kube-dns clusterIP: 192.1.0.100 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP 注意：clusterIP与node节点的kubelet配置文件cluster-dns的值保持一致 创建kubedns-controller.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: # replicas: not specified here: # 1. In order to make Addon Manager do not reconcile this replicas parameter. # 2. Default is 1. # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: rollingUpdate: maxSurge: 10% maxUnavailable: 0 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: tolerations: - key: "CriticalAddonsOnly" operator: "Exists" volumes: - name: kube-dns-config configMap: name: kube-dns optional: true containers: - name: kubedns image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4 resources: # TODO: Set memory limits when we've profiled the container for large # clusters, then set request = limit to keep this container in # guaranteed class. Currently, this container falls into the # "burstable" category so the kubelet doesn't backoff from restarting it. limits: memory: 170Mi requests: cpu: 100m memory: 70Mi livenessProbe: httpGet: path: /healthcheck/kubedns port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /readiness port: 8081 scheme: HTTP # we poll on pod startup for the Kubernetes master service and # only setup the /readiness HTTP server once that's available. initialDelaySeconds: 3 timeoutSeconds: 5 args: - --domain=cluster.local. - --dns-port=10053 - --config-dir=/kube-dns-config - --v=2 env: - name: PROMETHEUS_PORT value: "10055" ports: - containerPort: 10053 name: dns-local protocol: UDP - containerPort: 10053 name: dns-tcp-local protocol: TCP - containerPort: 10055 name: metrics protocol: TCP volumeMounts: - name: kube-dns-config mountPath: /kube-dns-config - name: dnsmasq image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4 livenessProbe: httpGet: path: /healthcheck/dnsmasq port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - -v=2 - -logtostderr - -configDir=/etc/k8s/dns/dnsmasq-nanny - -restartDnsmasq=true - -- - -k - --cache-size=1000 - --log-facility=- - --server=/cluster.local/127.0.0.1#10053 - --server=/in-addr.arpa/127.0.0.1#10053 - --server=/ip6.arpa/127.0.0.1#10053 ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP # see: https://github.com/kubernetes/kubernetes/issues/29055 for details resources: requests: cpu: 150m memory: 20Mi volumeMounts: - name: kube-dns-config mountPath: /etc/k8s/dns/dnsmasq-nanny - name: sidecar image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4 livenessProbe: httpGet: path: /metrics port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - --v=2 - --logtostderr - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A ports: - containerPort: 10054 name: metrics protocol: TCP resources: requests: memory: 20Mi cpu: 10m dnsPolicy: Default # Don't use cluster DNS. serviceAccountName: kube-dns 2.启动dns与测试 通过下面命令启动dns 12kubectl create -f kubedns-controller.yamlkubectl create -f kubedns-svc.yaml 并通过下面命令查看启动状态 123kubectl get pod [16:30:29]NAME READY STATUS RESTARTS AGEkube-dns-3097350089-q1hh1 3/3 Running 0 23h 能看到如上信息说明，dns已经成功启动 测试教程见测试dns 遇过的那些坑1.创建kubeconfig文件遇到后台日志一直提示server ask client for certificate的问题？ 解决方案：原来users元素中的name的值为server，而context中的user配置值为Ubuntu，因此，一直拿不到ubutux用户的秘钥信息。将server改为Ubuntu（注意这个文件的里面的层级关系） 2.在配置dns，API server一直反复提示：invalid token信息？ 解决方案：查阅了相关资料发现，API server认证过程中涉及的Pod涉及三个文件分别是token、ca.crt和namespace。因为之前在配置过程生成错误证书，也生成错误token；在配置好正确的证书后，API server依然沿用旧的token，因此将旧的token删除后，重试就ok. 安装配置一点点经验在手动安装kubernetes集群的时候，很多时候会遇到各种坑各种问题，根据问题去搜索的时候很难能寻找到答案。这时候可以通过以下进行排查： 1.查看相关日志进行排查 2.重新核对配置文件的配置信息 3.阅读配置相关参数的意义，很多时候配置参数有相关规则，需要根据指定的规则去设置，不然很多问题都难以解决 4.建议看官方文档，这是较为靠谱的做法，官方文档也有相关配置说明 5.实在解决不了，只能直接看源码，根据报错日志进行追溯“问题源头” 参考文献 手动部署kubernetes kubectl客户端配置 dns flannel原理简析及安装]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>cluster</tag>
      </tags>
  </entry>
</search>